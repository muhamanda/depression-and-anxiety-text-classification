# -*- coding: utf-8 -*-
"""ML_Dicoding 1 (Advance).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14yo6vMvCbiTNZwOn7QBcBnuemMgwTHkj

### **Nama: Muhammad Amanda**
### **Username: muh_amanda** 
### **Kota: Kota Jakarta Timur, DKI Jakarta**
### *Waktu bergabung pada 12 May 2020*

# Proyek Pertama : Membuat Model NLP dengan TensorFlow

Dataset : [Depression and Anxiety in Twitter (ID)](https://www.kaggle.com/stevenhans/depression-and-anxiety-in-twitter-id)
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!kaggle datasets download -d stevenhans/depression-and-anxiety-in-twitter-id

!ls

#unzipping the zip files and deleting the zip files
!unzip \*.zip  && rm *.zip

"""# **Import Dataset**

file data yang digunakan datd_train.csv dengan 2000+ baris dan 2 label/kelas.
"""

import pandas as pd
df = pd.read_csv('datd_train.csv')
df.info()

"""# **one-hot-encoding**

label kita berupa data kategorikal, maka kita perlu melakukan proses one-hot-encoding dan membuat dataframe baru.
"""

category = pd.get_dummies(df["label"])
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='label')
df_baru.head()

# mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array
tweet = df_baru['text'].values
label = df_baru[[0,1]].values

"""# Split Dataset

membagi dataset menjadi data train (80%) dan data test (20%).
"""

from sklearn.model_selection import train_test_split
tweet_latih, tweet_test, label_latih, label_test = train_test_split(tweet, label, test_size=0.2, stratify=label, random_state=42)

"""# **Tokenisasi**"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(tweet_latih) 
tokenizer.fit_on_texts(tweet_test)

"""# **Sekuens**"""

sekuens_latih = tokenizer.texts_to_sequences(tweet_latih)
sekuens_test = tokenizer.texts_to_sequences(tweet_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

"""# **Arsitektur model**
* menggunakan Embedding.
* menggunakan LSTM.
* menggunakan model sequential.
* mengimplementasikan callback.
"""

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(2, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

## membuat kelas callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.76) and (logs.get('val_accuracy')>0.76):
      print("\nAkurasi telah mencapai >75%!") #untuk ditampilkan ketika callback aktif.
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 200
history = model.fit(padded_latih, label_latih, epochs=num_epochs, 
                    validation_data=(padded_test, label_test), verbose=2,
                    callbacks=[callbacks],
                    batch_size=40,
                    steps_per_epoch=40)

"""# **Plot loss dan akurasi**"""

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

model.evaluate(padded_test, label_test)